{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pipelines these days\n",
    "\n",
    "From @wrobstory: the SIMPLE pipelines:\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/simplepipe.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why learn databases\n",
    "\n",
    "- you shouldnt really implement one\n",
    "- very hard to get right, many edge cases: eg: crash, fs outta space\n",
    "- but understanding how one works is good as, data storage/munging are not just database concerns, Eg: columnar storage in Apache Parquet. And arrow, the language agnostic dataframe\n",
    "- you do need to choose a storage engine that is ok for your program, and tuning a database requires deep knowledge of the correspondence between whats the output of `explain` and whats the structure of the db\n",
    "- in particular architectures that work for transaction processing are not aptimal for analytics\n",
    "- you will use database libraries as opposed to daemons: leveldb (Google Chrome), lmdb, bdb, sqlite (uses btrees for implementation)\n",
    "- a lot of what you have learnt comes together here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Relational vs Document\n",
    "\n",
    "- today both are highly used: we have *polyglot persistence*\n",
    "\n",
    "\n",
    "- Mongo/Couch/etc are document oriented, store JSON documents (hierarchical structure - hard to map to rows and columns)\n",
    "\n",
    "\n",
    "- these have a higher locality of data: its like a really wide row with hierarchy\n",
    "- JSON document - very local - everything in one place specific to person \n",
    "- standard relational db will have tables for everthing in separate places\n",
    "\n",
    "\n",
    "- normalization vs denormalization (put all data out and put it in one long big row)\n",
    "- never repeat anything\n",
    "- if want transactional, want to only have to update in one place\n",
    "\n",
    "\n",
    "- last 5-6 years have been explosion of databases, the standard relational model wasn't working for them\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relational Model\n",
    "\n",
    "- a relation (table) is a collection of tuples\n",
    "- SQL a declarative model: a query optimizer decides how to execute the query (if a field range covers 80% of values, should we use the index or the table?). Also parallelizable\n",
    "- *shredding* splits a document into multiple tables due to normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SQL and pandas - notions of tables\n",
    "# samples in rows \n",
    "# fields in columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Model\n",
    "\n",
    "- stores nested records\n",
    "- bad for many-to-many relationships.. many customer buys many products (documents all over the place, one doc for each transaction, one doc for each person, for each product - make relationships really complicated)\n",
    "- storage locality good for access, bad for writing\n",
    "- couch, mongo, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components to a database\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/dbmscomponents.png) (DBMS components from Hellerstein at al: Architecture of a Database System: circa 2007)\n",
    "\n",
    "- client connection manager: what to do with incomings\n",
    "\n",
    "- transactional storage\n",
    "    - storage data structures\n",
    "    - transactions and ACID: atomicity, consistency, isolation, durability\n",
    "    - atomicity: if update one thing like transfer money, then better withdraw from one account and deposit into another\n",
    "    - isolation: if multiple readers and writers access at same time, then only one can go through\n",
    "    - durability: database needs to be in a sensible state\n",
    "    - consistency: consistent state before and after.... not that important\n",
    "- process model: coroutines, threads, processes\n",
    "- query model and language: query optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whats the history of a database query\n",
    "\n",
    "From Hellerstein et al:\n",
    ">At the base of the gate agent’s query plan, one or more operators exist to request data from the database. These operators make calls to fetch data from the DBMS’ Transactional Storage Manager, which man- ages all data access (read) and manipulation (create, update, delete) calls. The storage system includes algorithms and data structures for organizing and accessing data on disk (“access methods”), including basic structures like tables and indexes. It also includes a buffer management module that decides when and what data to transfer between disk and memory buffers. Returning to our example, in the course of accessing data in the access methods, the gate agent’s query must invoke the transaction management code to ensure the well-known “ACID” properties of transactions . Before accessing data, locks are acquired from a lock manager to ensure correct execution in the face of other concurrent queries.\n",
    "\n",
    ">At this point in the example query’s life, it has begun to access data records, and is ready to use them to compute results for the client. This is done by “unwinding the stack” of activities we described up to this point. The access methods return control to the query executor’s operators, which orchestrate the computation of result tuples from database data; as result tuples are generated, they are placed in a buffer for the client communications manager, which ships the results back to the caller. For large result sets, the client typically will make additional calls to fetch more data incrementally from the query, resulting in multiple itera- tions through the communications manager, query execu- tor, and storage manager. In our simple example, at the end of the query the transaction is completed and the connec- tion closed; this results in the transaction manager cleaning up state for the transaction, the process manager freeing any control structures for the query, and the communi- cations manager cleaning up communication state for the connection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transaction Processing or Analytics?\n",
    "\n",
    "- Also known as OLTP vs OLAP/Warehousing\n",
    "- small query size vs aggregates over large ones\n",
    "- random writes from user input vs ordered ETL/stream\n",
    "- end user (amazon site) vs analyst (you)\n",
    "- GB to TB vs TB to PB\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/ETL.png)\n",
    "\n",
    "(from designing data intensive applications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling the different workloads\n",
    "\n",
    "- for smaller sizes any relational db will do\n",
    "- currently vendors focus on one or the other, not both\n",
    "- MS and SAP HANA support both but with different storage engines\n",
    "- OLTP need to be highly available, low latency\n",
    "- SQL (or any Pandasish syntax) is good for drilling down\n",
    "- warehousing: star schema with very wide fact table, but typically you focus on few columns at a time\n",
    "- btree indexes for oltp, bitmaps + btree for warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column oriented storage\n",
    "\n",
    "- store values from each column together in separate storage\n",
    "- lends itself to compression with bitmap indexes and run-length encoding\n",
    "- this involves choosing an appropriate sort order\n",
    "- the index then can be the data (great for IN and AND queries): there is no pinters to \"elsewhere\"\n",
    "- compressed indexes can fit into cache and are usable by iterators\n",
    "- bitwise AND/OR can be done with vector processing\n",
    "- several different sort orders can be redundantly stored\n",
    "- writing is harder: updating a row touches many column files\n",
    "- but you can write an in-memory front sorted store (row or column), and eventually merge onto the disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cubes\n",
    "\n",
    "- Basically a histogram of counts in bins for multiple fields\n",
    "- can give you fast marginals and conditionals in any combination of dimensions\n",
    "- expensive to update so only used for warehousing\n",
    "- such histograms are used by query optimizers as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing and Databases\n",
    "\n",
    "- an additional structure derived from the primary data\n",
    "- however a clustered index may actually store the data\n",
    "- there is overhead on writes: indexes speed up queries but slow down writes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple start\n",
    "\n",
    "- start with index for key-value data\n",
    "- aka dictionary\n",
    "- in memory you are done. the index IS the database\n",
    "- hash tables are no good for range queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# simplest in memory database\n",
    "# also an INDEX \n",
    "\n",
    "database=dict()\n",
    "database['rahul']=\"aged\"\n",
    "database['pavlos']=\"ancient\"\n",
    "database['kobe']=\"stillyoung\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stillyoung'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database['kobe']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing it on disk\n",
    "\n",
    "- in the hashmap(dict) in memory, store a file offset instead\n",
    "- this file is an append only file.\n",
    "- if you update, simply append a new entry and change the offset in the hashmap\n",
    "- this is what bitcask in Riak does\n",
    "- break the file into segments. Each segment, once written, the kv pairs are never changed. maintain a hashmap per segment and search these in order\n",
    "- run compaction to throw away dupes from segments and merge them; delete old files\n",
    "- deletion is done by writing a tombstone record\n",
    "- only one writer thread. \n",
    "- bitcask will store hashmap snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# after value_sz, read however many bytes to get value\n",
    "# keep appending stuff at end\n",
    "# but file will grow huge over time\n",
    "# keep these files time bound so that there's 1 a day \n",
    "# Then need to run a merge process for all these files\n",
    "\n",
    "# storing value_pos as offset\n",
    "\n",
    "# but now must also store the file where putting it in \n",
    "# this is what nosql does -- stores the file id and position of corresponding value\n",
    "\n",
    "# this is a memory based structure \n",
    "\n",
    "# if did not store info to file periodically, or on every write, would get into trouble, store all this stuff in memory \n",
    "#  and something will get lost\n",
    "# need to make sure you persist it \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://dl.dropboxusercontent.com/u/75194/riak1.png)\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/riak2.png)\n",
    "\n",
    "(from riak bitcask intro at http://basho.com/wp-content/uploads/2015/05/bitcask-intro.pdf )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import sys\n",
    "class Database():\n",
    "    \n",
    "    def __init__(self, file):\n",
    "        self.file = file\n",
    "        self.byteorder=sys.byteorder\n",
    "        if not os.path.exists(file):\n",
    "            self.fd = open(file, \"xb+\", buffering=0)\n",
    "            self.index={}\n",
    "        else:\n",
    "            self.fd = open(file, \"r+b\", buffering=0)\n",
    "            with open(file+\".idx\") as fdi:\n",
    "                items = [l.strip().split(':') for l in fdi.readlines()]\n",
    "                self.index = {k:int(v) for k,v in items}\n",
    "        self.readptr = self.fd.tell()\n",
    "        self.fd.seek(0,2)\n",
    "        self.writeptr = self.fd.tell()\n",
    "        \n",
    "        \n",
    "    def set(self, x, v):\n",
    "        if not isinstance(x, str):\n",
    "            raise ValueError(\"Key must be a string\")\n",
    "        bin_x = x.encode('utf-8')\n",
    "        sz_x=len(bin_x).to_bytes(1, byteorder=self.byteorder)\n",
    "        if not isinstance(v, str):\n",
    "            raise ValueError(\"Value must be a string\")\n",
    "        bin_v = v.encode('utf-8')\n",
    "        sz_v=len(bin_v).to_bytes(1, byteorder=self.byteorder)\n",
    "        try:\n",
    "            self.index[x]=self.writeptr\n",
    "            self.fd.seek(self.writeptr)\n",
    "            print(\"currently\", self.fd.tell())\n",
    "            self.fd.write(sz_x+sz_v+bin_x+bin_v)\n",
    "        except:\n",
    "            del self.index[x]\n",
    "        else:\n",
    "            self.writeptr=self.fd.tell()\n",
    "            \n",
    "    def get(self, x):\n",
    "        try:\n",
    "            offset = self.index[x]\n",
    "        except:\n",
    "            raise ValueError(\"{} is not in index\".format(x))\n",
    "        bin_x = x.encode('utf-8')\n",
    "        print(\"offset is\", offset)\n",
    "        self.readptr=offset\n",
    "        self.fd.seek(self.readptr)\n",
    "        sz_k = int.from_bytes(self.fd.read(1), byteorder=self.byteorder)\n",
    "        sz_v = int.from_bytes(self.fd.read(1), byteorder=self.byteorder)\n",
    "        self.fd.seek(sz_k,1)\n",
    "        readit=self.fd.read(sz_v).decode('utf-8')\n",
    "        print(\"now\", self.fd.tell())\n",
    "        return readit\n",
    "        \n",
    "    def close(self):\n",
    "        fdi=open(self.file+\".idx\",\"w\")\n",
    "        fdi.write(\"\\n\".join([k+\":\"+str(v) for k,v in self.index.items()]))\n",
    "        fdi.close()\n",
    "        self.fd.close()\n",
    "        \n",
    "    def __del__(self):\n",
    "        self.fd.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: /tmp/test.db: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!rm /tmp/test.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialize database with a .db file in the tmp directory\n",
    "db = Database(\"/tmp/test.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mKSOutOfProcessFetcher.0.ppfIhqX0vjaTSb8AJYobDV7Cu68=\u001b[m\u001b[m   \u001b[33mesets.gui.501.fifo\u001b[m\u001b[m\r\n",
      "\u001b[34mKSOutOfProcessFetcher.501.ppfIhqX0vjaTSb8AJYobDV7Cu68=\u001b[m\u001b[m \u001b[32mesets.sock\u001b[m\u001b[m\r\n",
      "\u001b[34mcom.apple.launchd.3Rh2hNC6ja\u001b[m\u001b[m                           \u001b[31mpcwifioccupy.filelock\u001b[m\u001b[m\r\n",
      "\u001b[34mcom.apple.launchd.8mX9YGkA8A\u001b[m\u001b[m                           test.db\r\n",
      "\u001b[34mcom.apple.launchd.Z1SYxwMDYP\u001b[m\u001b[m                           test.db.idx\r\n"
     ]
    }
   ],
   "source": [
    "# view the /tmp directory\n",
    "!ls /tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "# check the index attriute of db is empty dictionary\n",
    "print(db.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently 0\n",
      "currently 11\n",
      "currently 23\n"
     ]
    }
   ],
   "source": [
    "# set rahul as key and aged as value\n",
    "db.set(\"rahul\", \"aged\")\n",
    "db.set(\"pavlos\", \"aged\")\n",
    "db.set(\"kobe\", \"stillyoung\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pavlos': 11, 'rahul': 0, 'kobe': 23}\n"
     ]
    }
   ],
   "source": [
    "# get the index which should ahve keys and the file locattions\n",
    "print(db.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offset is 11\n",
      "now 23\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'aged'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get pavlos\n",
    "db.get(\"pavlos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently 39\n"
     ]
    }
   ],
   "source": [
    "# update rahul with a new word, just append it to the end (the old stuff is still there)\n",
    "db.set(\"rahul\",\"young\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pavlos': 11, 'rahul': 39, 'kobe': 23}\n"
     ]
    }
   ],
   "source": [
    "# get the updated inex\n",
    "print(db.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offset is 23\n",
      "now 39\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'stillyoung'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# our reader is sitll at 39\n",
    "db.get(\"kobe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offset is 39\n",
      "now 51\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'young'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# our read pointer is moving around. Once it gets the last item, it will stop at the end of the last item\n",
    "db.get(\"rahul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offset is 11\n",
      "now 23\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'aged'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.get(\"pavlos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kobe': 23, 'pavlos': 11, 'rahul': 39}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently 51\n"
     ]
    }
   ],
   "source": [
    "db.set(\"kobe\", \"retired\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kobe': 51, 'pavlos': 11, 'rahul': 39}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offset is 39\n",
      "now 51\n",
      "young\n",
      "offset is 11\n",
      "now 23\n",
      "aged\n",
      "offset is 51\n",
      "now 64\n",
      "retired\n"
     ]
    }
   ],
   "source": [
    "# move around the file to get items\n",
    "print(db.get(\"rahul\"))\n",
    "print(db.get(\"pavlos\"))\n",
    "print(db.get(\"kobe\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently 64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'kobe': 51, 'obama': 64, 'pavlos': 11, 'rahul': 39}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write new key-value at the last location left off\n",
    "db.set(\"obama\",\"president\")\n",
    "db.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offset is 39\n",
      "now 51\n",
      "young\n",
      "offset is 11\n",
      "now 23\n",
      "aged\n",
      "offset is 51\n",
      "now 64\n",
      "retired\n",
      "offset is 64\n",
      "now 80\n",
      "president\n"
     ]
    }
   ],
   "source": [
    "print(db.get(\"rahul\"))\n",
    "print(db.get(\"pavlos\"))\n",
    "print(db.get(\"kobe\"))\n",
    "print(db.get(\"obama\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offset is 39\n",
      "now 51\n",
      "young\n",
      "offset is 11\n",
      "now 23\n",
      "aged\n",
      "offset is 51\n",
      "now 64\n",
      "retired\n",
      "offset is 64\n",
      "now 80\n",
      "president\n"
     ]
    }
   ],
   "source": [
    "db=Database(\"/tmp/test.db\")\n",
    "print(db.get(\"rahul\"))\n",
    "print(db.get(\"pavlos\"))\n",
    "print(db.get(\"kobe\"))\n",
    "print(db.get(\"obama\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently 80\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'kobe': 51, 'obama': 64, 'pavlos': 80, 'rahul': 39}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.set(\"pavlos\", \"ancient\")\n",
    "db.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offset is 39\n",
      "now 51\n",
      "young\n",
      "offset is 80\n",
      "now 95\n",
      "ancient\n",
      "offset is 51\n",
      "now 64\n",
      "retired\n",
      "offset is 64\n",
      "now 80\n",
      "president\n"
     ]
    }
   ],
   "source": [
    "print(db.get(\"rahul\"))\n",
    "print(db.get(\"pavlos\"))\n",
    "print(db.get(\"kobe\"))\n",
    "print(db.get(\"obama\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# close the database\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pavlos:80\r\n",
      "rahul:39\r\n",
      "kobe:51\r\n",
      "obama:64"
     ]
    }
   ],
   "source": [
    "# print out what's in the idx file\n",
    "!cat /tmp/test.db.idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rahulaged\r\n",
      "pavlosaged\r\n",
      "kobestillyoung\r\n",
      "rahulyoung\r\n",
      "koberetired\r\n",
      "obamapresident\r\n",
      "pavlosancient\r\n"
     ]
    }
   ],
   "source": [
    "# TODO strings?\n",
    "# get strings of what's int the db file\n",
    "!strings /tmp/test.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# range search\n",
    "# use a tree\n",
    "# or use a sorted array if can figure out how to sort an array in a reasonable fashion\n",
    "\n",
    "# Store file every 10,000 transactions\n",
    "# Need to store - hashtables don't sort \n",
    "# Use a tree \n",
    "# in memory, store into a tree as they come in, then get an automatically sorted structure in n log n\n",
    "# then write in this structure every so often\n",
    "\n",
    "# now you lost temporal access of things\n",
    "# sorted in time order, so now you've lost things???\n",
    "# Lost something that I had earlier, which was this notion of after storing file offsets as to where things were, \n",
    "# I knew which was the latest one and go one by one and find \n",
    "# now each file going to be sorted a-z. So if need to find a thing, might get at latest one \n",
    "# need to do something in-order traversal\n",
    "\n",
    "\n",
    "# time order files in chunk, but they are sorted\n",
    "# Ok for a segment, create a segment instead... not a file id\n",
    "\n",
    "\n",
    "# Use a bloom filter \n",
    "# use a memory structure in front to check if it's in there or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting more sophisticated\n",
    "\n",
    "### SSTables and LSM trees\n",
    "\n",
    "- keep the segments from last time\n",
    "- now add the requirement that these are sorted by key\n",
    "- merging segments is like mergesort; track recentness \n",
    "- now all keys need not be in memory, only some"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://dl.dropboxusercontent.com/u/75194/sstable.png)\n",
    "\n",
    "(from designing data intensive applications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- how to maintain the sort?\n",
    "- well maintain it in memory using a balanced binary tree or a memtable\n",
    "- once in-mem struct exceeds a certain size, flush to disk\n",
    "- for crashes, keep a log per memtable, to be discarded when memtable is written to a sstable. Each write is immediately appended, this is like a WAL.\n",
    "- again do lookups most recent first, and do compaction and merging. A high throughput on writes will affect compactions and vice versa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- called a LSM tree\n",
    "- popularized by bigtable\n",
    "- used in cassandra, riak, hbase, leveldb, rocksdb\n",
    "- indeed in lucent where the value is a list of documents\n",
    "- you could use it for vocabularies in your NLP.\n",
    "- leveldb uses bloom filters to prevent multiple searches if not there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To deal with crashes, keep a log, so can regenerate memtable from log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The most common indexing structure, btree\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/btree1q.png)\n",
    "\n",
    "(from https://loveforprogramming.quora.com/Memory-locality-the-magic-of-B-Trees)\n",
    "\n",
    "- \"A linked sorted distributed range array with predefined sub array size which allows searches, sequential access, insertions, and deletions in logarithmic time. \"\n",
    "- it is a generalization of a binary tree\n",
    "- but the branching factor is much higher, and the depth thus smaller\n",
    "- brees break database into pages, and read-or-write one page at a time\n",
    "- leaf pages contain all the values and may represent a clustered index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://dl.dropboxusercontent.com/u/75194/btree1.png)\n",
    "(from designing data intensive applications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we update a key, a split can happen\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/btree2.png)\n",
    "\n",
    "(from designing data intensive applications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an in-place modificaltion unlike what we had earlier. The data structure is mutable. This can cause issues for transactions, and must be dealt with. One can create immutable b-trees, and lmdb, the database inside of openldap, does this. It uses a copy-on-write schee which writes new pages elsewhere\n",
    "\n",
    "Both splits and writing in-place are dangerous, so its normal for b-tree implementations to have a WAL, or write ahead log (such a log can also be used to manage transactions). Every operation on the btree is appended to this log file.\n",
    "\n",
    "In B+ trees, pointers amonst the leaf nodes make for an easier linear scan.\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/btree2q.png)\n",
    "\n",
    "(from https://loveforprogramming.quora.com/Memory-locality-the-magic-of-B-Trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Immutable BST\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/immutablebst.png)\n",
    "\n",
    "(from purely functional data structures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B-tree vs LSM tree\n",
    "\n",
    "- comparable on random reads\n",
    "- LSM tree good on random writes as it makes the writes sequential\n",
    "- B-tree good for transactions; at most one place there things are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other indexes\n",
    "\n",
    "- key-value indexes suppose unique keys and are thus like primary keys in a relational model\n",
    "- you can also have secondary keys or multi-column keys which can be created by some kind of concatenation or a multi-dimensional r-tree, \n",
    "- in a k-v database, the value is stored in the index and we are done. Usually in rdbms, the rows are stored in a heap file to avoid duplication\n",
    "- in mysql's innodb engine, the pk is a clustered index, while the secondary keys point to the pk.\n",
    "- a covering index stores certain columns in the index. Of-course in a columnar situation, the column is the index when we choose that columns ordering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## in-memory databases\n",
    "\n",
    "- are fast as byte serialization is not needed\n",
    "- in the anti-caching pattern evice LRU data to disk like virtual memory or swapping, but managed by the db. This means that you can now have an in-memory database which can handle out-of-core data.\n",
    "- h-store (now voltdb) uses this in-mamory idea with single threading to achieve high throughput in a OLTP scenario, relying on many partitions for ACID."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Today's exercises\n",
    "\n",
    "1. Implement deletion (to submit next monday)\n",
    "2. Think about concurrency issues inour little database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amy's Lab Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO copy to lab \n",
    "\n",
    "# this is a slow implementation\n",
    "# right and left pointer are the same\n",
    "# ordinarily, would want 2 pointers - read and write\n",
    "# append-only database\n",
    "# keeps index in memory\n",
    "# appends this to disk as persistent storage\n",
    "# writes that pipeline dictionary to disk\n",
    "# has offsets in memory\n",
    "# run this \n",
    "# add a delete to this \n",
    "# understand how it works\n",
    "# make sure you understand how it works \n",
    "\n",
    "# deletion of labs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing it on disk\n",
    "\n",
    "- in the hashmap(dict) in memory, store a file offset instead\n",
    "- this file is an append only file.\n",
    "- if you update, simply append a new entry and change the offset in the hashmap\n",
    "- this is what bitcask in Riak does\n",
    "- break the file into segments. Each segment, once written, the kv pairs are never changed. maintain a hashmap per segment and search these in order\n",
    "- run compaction to throw away dupes from segments and merge them; delete old files\n",
    "- deletion is done by writing a tombstone record\n",
    "- only one writer thread. \n",
    "- bitcask will store hashmap snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Whole idea is that this is an append only database! \n",
    "# keep file pointer stuck at the end of the file\n",
    "\n",
    "import os.path\n",
    "import sys\n",
    "class Database():\n",
    "    \n",
    "    def __init__(self, file):\n",
    "        # create a file\n",
    "        self.file = file\n",
    "        \n",
    "        # TODO what is this? \n",
    "        # An indicator of the native byte order. This will have the value 'big' on big-endian (most-significant byte \n",
    "        # first) platforms, and 'little' on little-endian (least-significant byte first) platforms.\n",
    "        self.byteorder=sys.byteorder\n",
    "        # if file doesn't exist, then open it and set up new index\n",
    "        if not os.path.exists(file):\n",
    "            self.fd = open(file, \"xb+\", buffering=0)\n",
    "            self.index={}\n",
    "        # if file does exist, then \n",
    "        else:\n",
    "            # file handler\n",
    "            self.fd = open(file, \"r+b\", buffering=0)\n",
    "            with open(file+\".idx\") as fdi:\n",
    "                items = [l.strip().split(':') for l in fdi.readlines()]\n",
    "                self.index = {k:int(v) for k,v in items}\n",
    "                \n",
    "        # read pointer\n",
    "        # tell() returns the current position of the file read/write pointer within the file.\n",
    "        # set readpointer to the beginning of the file\n",
    "        self.readptr = self.fd.tell()\n",
    "        \n",
    "        # seek() sets the file's current position at the offset\n",
    "        # 0 arg: offset -- This is the position of the read/write pointer within the file.\n",
    "        # 2 arg (the 2nd one): whence This is optional and defaults to 0 which means absolute file positioning, \n",
    "            # other values are 1 which means seek relative to the current position and 2 means seek relative \n",
    "            # to the file's end.\n",
    "        self.fd.seek(0,2)\n",
    "        \n",
    "        # set write pointer to the end of the file\n",
    "        self.writeptr = self.fd.tell()\n",
    "        \n",
    "        \n",
    "    def set(self, x, v):\n",
    "        # check key is a string\n",
    "        if not isinstance(x, str):\n",
    "            raise ValueError(\"Key must be a string\")\n",
    "        # encode the key to utf-8\n",
    "        bin_x = x.encode('utf-8')\n",
    "        # store the size in bytes of encoded key\n",
    "        sz_x=len(bin_x).to_bytes(1, byteorder=self.byteorder)\n",
    "        \n",
    "        # ensure value must be a string\n",
    "        if not isinstance(v, str):\n",
    "            raise ValueError(\"Value must be a string\")\n",
    "        # encode value to utf-8\n",
    "        bin_v = v.encode('utf-8')\n",
    "        # store the size of the bytes of encoded value\n",
    "        sz_v=len(bin_v).to_bytes(1, byteorder=self.byteorder)\n",
    "\n",
    "        # set the writepointer in the index\n",
    "        try:\n",
    "            # set the index to the writeptr location of where the file will be written\n",
    "            self.index[x]=self.writeptr\n",
    "            # go to the writeptr location in the file (using seek)\n",
    "            self.fd.seek(self.writeptr)\n",
    "            # print where the pointer currently is in the file\n",
    "            print(\"currently\", self.fd.tell())\n",
    "            \n",
    "            # write len of key + len of value + encoded key + encoded value\n",
    "            self.fd.write(sz_x+sz_v+bin_x+bin_v)\n",
    "        except:\n",
    "            # if it doesn't work, delete x\n",
    "            del self.index[x]\n",
    "        else:\n",
    "            # update writeptr to new location\n",
    "            self.writeptr=self.fd.tell()\n",
    "            \n",
    "    def get(self, x):\n",
    "        # get the offset in the file\n",
    "        try:\n",
    "            offset = self.index[x]\n",
    "        except:\n",
    "            raise ValueError(\"{} is not in index\".format(x))\n",
    "        \n",
    "        # encode x \n",
    "        bin_x = x.encode('utf-8')\n",
    "        print(\"offset is\", offset)\n",
    "        \n",
    "        # set the readptr to the offset \n",
    "        self.readptr=offset\n",
    "        # go to that location in the file \n",
    "        self.fd.seek(self.readptr)\n",
    "        \n",
    "        # TODO what is this doing? \n",
    "        # get the len of int \n",
    "        sz_k = int.from_bytes(self.fd.read(1), byteorder=self.byteorder)\n",
    "        sz_v = int.from_bytes(self.fd.read(1), byteorder=self.byteorder)\n",
    "        \n",
    "        # seek the file the length of an int? Should be length of the key\n",
    "        self.fd.seek(sz_k,1)\n",
    "        \n",
    "        # read from the seek location \n",
    "        readit=self.fd.read(sz_v).decode('utf-8')\n",
    "        print(\"now\", self.fd.tell())\n",
    "        return readit\n",
    "        \n",
    "    def delete(self, x):\n",
    "        \"\"\"\n",
    "        For my deletion flag I use the code '-999'\n",
    "        \"\"\"\n",
    "        # My code here \n",
    "        # first check if key exists in the index\n",
    "        try: \n",
    "            self.get(x)\n",
    "        except ValueError: \n",
    "            raise ValueError(\"{} is not in index\".format(x))\n",
    "        \n",
    "        # append to database with tombostone deletion flag. No need to delete the previous writes \n",
    "        self.set(x, '-999')\n",
    "        # no need to delete from index\n",
    "        return    \n",
    "    \n",
    "    def close_db(self):\n",
    "        # open file, write to file, and close file\n",
    "        fdi=open(self.file+\".idx\",\"w\")\n",
    "        fdi.write(\"\\n\".join([k+\":\"+str(v) for k,v in self.index.items()]))\n",
    "        fdi.close()\n",
    "        # TODO what is this doing? \n",
    "        self.fd.close()        \n",
    "        \n",
    "    # this is the database object deletion... not key-value deletion!\n",
    "    def __del__(self):\n",
    "        self.fd.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rm /tmp/amy.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialize database with a .db file in the tmp directory\n",
    "db = Database(\"/tmp/amy.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "print(db.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently 0\n",
      "currently 11\n",
      "currently 23\n"
     ]
    }
   ],
   "source": [
    "db.set(\"rahul\", \"aged\")\n",
    "db.set(\"pavlos\", \"aged\")\n",
    "db.set(\"kobe\", \"stillyoung\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pavlos': 11, 'rahul': 0, 'kobe': 23}\n"
     ]
    }
   ],
   "source": [
    "print(db.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offset is 0\n",
      "now 11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'aged'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.get('rahul')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "amy is not in index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-57e268bf615d>\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'amy'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-d8aa15429154>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'amy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-75-57e268bf615d>\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{} is not in index\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;31m# encode x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: amy is not in index"
     ]
    }
   ],
   "source": [
    "db.get('amy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offset is 0\n",
      "now 11\n",
      "currently 39\n"
     ]
    }
   ],
   "source": [
    "db.delete('rahul')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pavlos': 11, 'rahul': 39, 'kobe': 23}\n"
     ]
    }
   ],
   "source": [
    "print(db.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db.close_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pavlos:11\r\n",
      "rahul:39\r\n",
      "kobe:23"
     ]
    }
   ],
   "source": [
    "!cat /tmp/amy.db.idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rahulaged\r\n",
      "pavlosaged\r\n",
      "kobestillyoung\r\n",
      "rahul-999\r\n"
     ]
    }
   ],
   "source": [
    "!strings /tmp/amy.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
